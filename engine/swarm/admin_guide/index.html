<!-- Page generated 2021-11-17 12:16:07 +0000-->
<!DOCTYPE html>
<html lang="en"><head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-WL2QLG5');</script>
<title>Administer and maintain a swarm of Docker Engines | Docker Documentation</title>
  <meta name="description" content="Manager administration guide" />
  <meta name="keywords" content="docker, container, swarm, manager, raft">
  <link rel="canonical" href="https://docs.docker.com/engine/swarm/admin_guide/" />

  <!-- favicon -->
  <link rel="icon" type="image/x-icon" href="/favicons/docs@2x.ico" sizes="129x128">
  <link rel="apple-touch-icon" type="image/x-icon" href="/favicons/docs@2x.ico" sizes="129x128">
  <meta name="msapplication-TileImage" content="/favicons/docs@2x.ico">
  <meta property="og:image" content="/favicons/docs@2x.ico"/>
  <meta name="theme-color" content="#2496ed" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- hide elements that are only shown without JavaScript enabled -->
  <script>document.documentElement.classList.add('js')</script>
  <style>html.js .no-js { display: none !important; }</style><script defer src="/js/theme-switcher.js"></script>
  <script defer src="/js/anchorlinks.js"></script>
  <script defer src="/js/jquery.js"></script>
  <script defer src="/js/bootstrap.min.js"></script>
  <script defer src="/js/docs.js"></script><script defer src="/js/search.js"></script><link rel="preload" as="font" href="https://fonts.gstatic.com/s/opensans/v18/mem8YaGs126MiZpBA-UFVZ0bf8pkAg.woff2" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" as="font" href="/fonts/geomanist/hinted-Geomanist-Book.woff2"    type="font/woff2" crossorigin="anonymous">
  <link rel="preload" as="font" href="/fonts/geomanist/hinted-Geomanist-Regular.woff2" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" as="font" href="/fonts/glyphicons-halflings-regular.woff2"       type="font/woff2" crossorigin="anonymous">
  <link rel="preload" as="font" href="/fonts/fontawesome-webfont.woff2?v=4.7.0"        type="font/woff2" crossorigin="anonymous">

  <link rel="stylesheet" href="/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/bootstrap.min.css">
  <link rel="stylesheet" href="/css/style.css" id="pagestyle">

  <!-- SEO stuff -->
  <meta name="twitter:title" itemprop="title name" content="Administer and maintain a swarm of Docker Engines"/>
  <meta name="twitter:description" property="og:description" itemprop="description" content="" />
  <meta name="twitter:card" content="summary_large_image"/>
  <meta name="twitter:domain" content="docs.docker.com"/>
  <meta name="twitter:site" content="@docker_docs"/>
  <meta name="twitter:url" content="https://twitter.com/docker_docs"/>
  <meta name="twitter:image:src" content="https://docs.docker.com/images/docs@2x.png"/>
  <meta name="twitter:image:alt" content="Docker Documentation"/>
  <meta property="og:title" content="Administer and maintain a swarm of Docker Engines" />
  <meta property="og:description" content="Manager administration guide" />
  <meta property="og:type" content="website"/>
  <meta property="og:updated_time" itemprop="dateUpdated" content="2021-11-17T12:16:07+00:00"/>
  <meta property="og:image" itemprop="image primaryImageOfPage" content="https://docs.docker.com/images/docs@2x.png"/>
  <meta property="og:locale" content="en_US" />
  <meta property="og:url" content="https://docs.docker.com/engine/swarm/admin_guide/" />
  <meta property="og:site_name" content="Docker Documentation" />
  <meta property="article:published_time" itemprop="datePublished" content="2021-11-17T12:16:07+00:00"/>
  <script type="application/ld+json">{"@context":"http://schema.org","@type":"WebPage","headline":"Administer and maintain a swarm of Docker Engines","description":"Manager administration guide","url":"https://docs.docker.com/engine/swarm/admin_guide/"}</script>
  <!-- END SEO STUFF -->
</head>
<body class="colums">
    <header>
        <nav class="nav-secondary navbar navbar-fixed-top">
    <div class="container-fluid">
        <div class="navbar-header">
            <a href="/">
                <img class="logo" src="/images/docker-docs-logo.svg" alt="Docker Docs" title="Docker Docs" width="160" height="28" />
            </a>
        </div>
        <div class="navbar-collapse" aria-expanded="false" style="height: 1px;">
            <div class="logo-mobile">
    <a href="/">
        <img src="/images/docker-icon.svg" alt="Docker Docs" title="Docker Docs" width="30" height="30" />
    </a>
</div>
<div class="search-form" id="search-div">
    <form class="search-form form-inline" id="searchForm" action="/search/" method="get">
        <label for="st-search-input" class="sr-only">Search</label>
        <input class="search-field form-control ds-input" id="st-search-input" name="q" placeholder="Search the docs" type="search" autocomplete="off" spellcheck="false" dir="auto" style="position: relative; vertical-align: top;">
        <div id="autocompleteResults"></div>
        <!-- <button type="submit" class="search-submit btn btn-default">Search</button> -->
    </form>
</div>
<div class="sidebar-toggle">
    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
    </button>
</div>
<div class="nav-container hidden-sm hidden-xs">
    <div>
        <ul class="nav navbar-nav"><li><a href="/" id="home">Home</a></li><li><a href="/get-started/overview/" id="guides">Guides</a></li><li><a href="/desktop/" id="manuals">Manuals</a></li><li><a href="/reference/" id="reference">Reference</a></li><li><a href="/samples/" id="samples">Samples</a></li></ul>
    </div>
    <div class="ctrl-right">
        <a href="javascript:void(0)" id="menu-toggle" aria-label="Current page's menu toggle"><i class="fa fa-indent" aria-hidden="true"></i></a>
    </div>
</div>
<div class="row hidden-sm hidden-xs">
    <nav aria-label="breadcrumb">
        <ol class="breadcrumb">
            <li><a href="/" title="Docker docs homepage"><i class="fa fa-home"></i></a></li>
            <li><a href="/get-started/overview/">Guides</a></li><li><a>Run your app in production</a></li><li><a href="/config/containers/start-containers-automatically/">Configure containers</a></li><li><a href="/engine/swarm/">Scale your app</a></li><li><a href="/engine/swarm/admin_guide/">Swarm administration guide</a></li></ol>
    </nav>
</div></div>
    </div>
</nav>

    </header>
    <div class="wrapper right-open">
        <div class="container-fluid">
            <div class="row">
                <div class="col-body">
                    <main class="col-content content">
                        <section class="section"><h1>Administer and maintain a swarm of Docker Engines</h1><p><em class="reading-time">Estimated reading time: 16 minutes</em></p><p>When you run a swarm of Docker Engines, <strong>manager nodes</strong> are the key components
for managing the swarm and storing the swarm state. It is important to
understand some key features of manager nodes to properly deploy and
maintain the swarm.</p>

<p>Refer to <a href="/engine/swarm/how-swarm-mode-works/nodes/">How nodes work</a>
for a brief overview of Docker Swarm mode and the difference between manager and
worker nodes.</p>

<h2 id="operate-manager-nodes-in-a-swarm">Operate manager nodes in a swarm</h2>

<p>Swarm manager nodes use the <a href="/engine/swarm/raft/">Raft Consensus Algorithm</a> to manage the
swarm state. You only need to understand some general concepts of Raft in
order to manage a swarm.</p>

<p>There is no limit on the number of manager nodes. The decision about how many
manager nodes to implement is a trade-off between performance and
fault-tolerance. Adding manager nodes to a swarm makes the swarm more
fault-tolerant. However, additional manager nodes reduce write performance
because more nodes must acknowledge proposals to update the swarm state.
This means more network round-trip traffic.</p>

<p>Raft requires a majority of managers, also called the quorum, to agree on
proposed updates to the swarm, such as node additions or removals. Membership
operations are subject to the same constraints as state replication.</p>

<h3 id="maintain-the-quorum-of-managers">Maintain the quorum of managers</h3>

<p>If the swarm loses the quorum of managers, the swarm cannot perform management
tasks. If your swarm has multiple managers, always have more than two.
To maintain quorum, a majority of managers must be available. An odd number of
managers is recommended, because the next even number does not make the quorum
easier to keep. For instance, whether you have 3 or 4 managers, you can still
only lose 1 manager and maintain the quorum. If you have 5 or 6 managers, you
can still only lose two.</p>

<p>Even if a swarm loses the quorum of managers, swarm tasks on existing worker
nodes continue to run. However, swarm nodes cannot be added, updated, or
removed, and new or existing tasks cannot be started, stopped, moved, or
updated.</p>

<p>See <a href="#recover-from-losing-the-quorum">Recovering from losing the quorum</a> for
troubleshooting steps if you do lose the quorum of managers.</p>

<h2 id="configure-the-manager-to-advertise-on-a-static-ip-address">Configure the manager to advertise on a static IP address</h2>

<p>When initiating a swarm, you must specify the <code class="highlighter-rouge">--advertise-addr</code> flag to
advertise your address to other manager nodes in the swarm. For more
information, see <a href="/engine/swarm/swarm-mode/#configure-the-advertise-address">Run Docker Engine in swarm mode</a>. Because manager nodes are
meant to be a stable component of the infrastructure, you should use a <em>fixed
IP address</em> for the advertise address to prevent the swarm from becoming
unstable on machine reboot.</p>

<p>If the whole swarm restarts and every manager node subsequently gets a new IP
address, there is no way for any node to contact an existing manager. Therefore
the swarm is hung while nodes try to contact one another at their old IP addresses.</p>

<p>Dynamic IP addresses are OK for worker nodes.</p>

<h2 id="add-manager-nodes-for-fault-tolerance">Add manager nodes for fault tolerance</h2>

<p>You should maintain an odd number of managers in the swarm to support manager
node failures. Having an odd number of managers ensures that during a network
partition, there is a higher chance that the quorum remains available to process
requests if the network is partitioned into two sets. Keeping the quorum is not
guaranteed if you encounter more than two network partitions.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Swarm Size</th>
      <th style="text-align: center">Majority</th>
      <th style="text-align: center">Fault Tolerance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: center">2</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>3</strong></td>
      <td style="text-align: center">2</td>
      <td style="text-align: center"><strong>1</strong></td>
    </tr>
    <tr>
      <td style="text-align: center">4</td>
      <td style="text-align: center">3</td>
      <td style="text-align: center">1</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>5</strong></td>
      <td style="text-align: center">3</td>
      <td style="text-align: center"><strong>2</strong></td>
    </tr>
    <tr>
      <td style="text-align: center">6</td>
      <td style="text-align: center">4</td>
      <td style="text-align: center">2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>7</strong></td>
      <td style="text-align: center">4</td>
      <td style="text-align: center"><strong>3</strong></td>
    </tr>
    <tr>
      <td style="text-align: center">8</td>
      <td style="text-align: center">5</td>
      <td style="text-align: center">3</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>9</strong></td>
      <td style="text-align: center">5</td>
      <td style="text-align: center"><strong>4</strong></td>
    </tr>
  </tbody>
</table>

<p>For example, in a swarm with <em>5 nodes</em>, if you lose <em>3 nodes</em>, you don’t have a
quorum. Therefore you can’t add or remove nodes until you recover one of the
unavailable manager nodes or recover the swarm with disaster recovery
commands. See <a href="#recover-from-disaster">Recover from disaster</a>.</p>

<p>While it is possible to scale a swarm down to a single manager node, it is
impossible to demote the last manager node. This ensures you maintain access to
the swarm and that the swarm can still process requests. Scaling down to a
single manager is an unsafe operation and is not recommended. If
the last node leaves the swarm unexpectedly during the demote operation, the
swarm becomes unavailable until you reboot the node or restart with
<code class="highlighter-rouge">--force-new-cluster</code>.</p>

<p>You manage swarm membership with the <code class="highlighter-rouge">docker swarm</code> and <code class="highlighter-rouge">docker node</code>
subsystems. Refer to <a href="/engine/swarm/join-nodes/">Add nodes to a swarm</a> for more information
on how to add worker nodes and promote a worker node to be a manager.</p>

<h3 id="distribute-manager-nodes">Distribute manager nodes</h3>

<p>In addition to maintaining an odd number of manager nodes, pay attention to
datacenter topology when placing managers. For optimal fault-tolerance, distribute
manager nodes across a minimum of 3 availability-zones to support failures of an
entire set of machines or common maintenance scenarios. If you suffer a failure
in any of those zones, the swarm should maintain the quorum of manager nodes
available to process requests and rebalance workloads.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Swarm manager nodes</th>
      <th style="text-align: center">Repartition (on 3 Availability zones)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: center">1-1-1</td>
    </tr>
    <tr>
      <td style="text-align: center">5</td>
      <td style="text-align: center">2-2-1</td>
    </tr>
    <tr>
      <td style="text-align: center">7</td>
      <td style="text-align: center">3-2-2</td>
    </tr>
    <tr>
      <td style="text-align: center">9</td>
      <td style="text-align: center">3-3-3</td>
    </tr>
  </tbody>
</table>

<h3 id="run-manager-only-nodes">Run manager-only nodes</h3>

<p>By default manager nodes also act as a worker nodes. This means the scheduler
can assign tasks to a manager node. For small and non-critical swarms
assigning tasks to managers is relatively low-risk as long as you schedule
services using <strong>resource constraints</strong> for <em>cpu</em> and <em>memory</em>.</p>

<p>However, because manager nodes use the Raft consensus algorithm to replicate data
in a consistent way, they are sensitive to resource starvation. You should
isolate managers in your swarm from processes that might block swarm
operations like swarm heartbeat or leader elections.</p>

<p>To avoid interference with manager node operation, you can drain manager nodes
to make them unavailable as worker nodes:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span> docker node update <span class="nt">--availability</span> drain &lt;NODE&gt;
</code></pre></div></div>

<p>When you drain a node, the scheduler reassigns any tasks running on the node to
other available worker nodes in the swarm. It also prevents the scheduler from
assigning tasks to the node.</p>

<h2 id="add-worker-nodes-for-load-balancing">Add worker nodes for load balancing</h2>

<p><a href="/engine/swarm/join-nodes/">Add nodes to the swarm</a> to balance your swarm’s
load. Replicated service tasks are distributed across the swarm as evenly as
possible over time, as long as the worker nodes are matched to the requirements
of the services. When limiting a service to run on only specific types of nodes,
such as nodes with a specific number of CPUs or amount of memory, remember that
worker nodes that do not meet these requirements cannot run these tasks.</p>

<h2 id="monitor-swarm-health">Monitor swarm health</h2>

<p>You can monitor the health of manager nodes by querying the docker <code class="highlighter-rouge">nodes</code> API
in JSON format through the <code class="highlighter-rouge">/nodes</code> HTTP endpoint. Refer to the
<a href="/engine/api/v1.25/#tag/Node">nodes API documentation</a>
for more information.</p>

<p>From the command line, run <code class="highlighter-rouge">docker node inspect &lt;id-node&gt;</code> to query the nodes.
For instance, to query the reachability of the node as a manager:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span> docker node inspect manager1 <span class="nt">--format</span> <span class="s2">"{{ .ManagerStatus.Reachability }}"</span>
<span class="go">reachable
</span></code></pre></div></div>

<p>To query the status of the node as a worker that accept tasks:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span> docker node inspect manager1 <span class="nt">--format</span> <span class="s2">"{{ .Status.State }}"</span>
<span class="go">ready
</span></code></pre></div></div>

<p>From those commands, we can see that <code class="highlighter-rouge">manager1</code> is both at the status
<code class="highlighter-rouge">reachable</code> as a manager and <code class="highlighter-rouge">ready</code> as a worker.</p>

<p>An <code class="highlighter-rouge">unreachable</code> health status means that this particular manager node is unreachable
from other manager nodes. In this case you need to take action to restore the unreachable
manager:</p>

<ul>
  <li>Restart the daemon and see if the manager comes back as reachable.</li>
  <li>Reboot the machine.</li>
  <li>If neither restarting or rebooting work, you should add another manager node or promote a worker to be a manager node. You also need to cleanly remove the failed node entry from the manager set with <code class="highlighter-rouge">docker node demote &lt;NODE&gt;</code> and <code class="highlighter-rouge">docker node rm &lt;id-node&gt;</code>.</li>
</ul>

<p>Alternatively you can also get an overview of the swarm health from a manager
node with <code class="highlighter-rouge">docker node ls</code>:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span> docker node <span class="nb">ls</span>
<span class="go">ID                           HOSTNAME  MEMBERSHIP  STATUS  AVAILABILITY  MANAGER STATUS
1mhtdwhvsgr3c26xxbnzdc3yp    node05    Accepted    Ready   Active
516pacagkqp2xc3fk9t1dhjor    node02    Accepted    Ready   Active        Reachable
9ifojw8of78kkusuc4a6c23fx *  node01    Accepted    Ready   Active        Leader
ax11wdpwrrb6db3mfjydscgk7    node04    Accepted    Ready   Active
bb1nrq2cswhtbg4mrsqnlx1ck    node03    Accepted    Ready   Active        Reachable
di9wxgz8dtuh9d2hn089ecqkf    node06    Accepted    Ready   Active
</span></code></pre></div></div>

<h2 id="troubleshoot-a-manager-node">Troubleshoot a manager node</h2>

<p>You should never restart a manager node by copying the <code class="highlighter-rouge">raft</code> directory from another node. The data directory is unique to a node ID. A node can only use a node ID once to join the swarm. The node ID space should be globally unique.</p>

<p>To cleanly re-join a manager node to a cluster:</p>

<ol>
  <li>To demote the node to a worker, run <code class="highlighter-rouge">docker node demote &lt;NODE&gt;</code>.</li>
  <li>To remove the node from the swarm, run <code class="highlighter-rouge">docker node rm &lt;NODE&gt;</code>.</li>
  <li>Re-join the node to the swarm with a fresh state using <code class="highlighter-rouge">docker swarm join</code>.</li>
</ol>

<p>For more information on joining a manager node to a swarm, refer to
<a href="/engine/swarm/join-nodes/">Join nodes to a swarm</a>.</p>

<h2 id="forcibly-remove-a-node">Forcibly remove a node</h2>

<p>In most cases, you should shut down a node before removing it from a swarm with
the <code class="highlighter-rouge">docker node rm</code> command. If a node becomes unreachable, unresponsive, or
compromised you can forcefully remove the node without shutting it down by
passing the <code class="highlighter-rouge">--force</code> flag. For instance, if <code class="highlighter-rouge">node9</code> becomes compromised:</p>

<pre><code class="language-none">$ docker node rm node9

Error response from daemon: rpc error: code = 9 desc = node node9 is not down and can't be removed

$ docker node rm --force node9

Node node9 removed from swarm
</code></pre>

<p>Before you forcefully remove a manager node, you must first demote it to the
worker role. Make sure that you always have an odd number of manager nodes if
you demote or remove a manager.</p>

<h2 id="back-up-the-swarm">Back up the swarm</h2>

<p>Docker manager nodes store the swarm state and manager logs in the
<code class="highlighter-rouge">/var/lib/docker/swarm/</code> directory. This data includes the keys used to encrypt
the Raft logs. Without these keys, you cannot restore the swarm.</p>

<p>You can back up the swarm using any manager. Use the following procedure.</p>

<ol>
  <li>
    <p>If the swarm has auto-lock enabled, you need the unlock key
to restore the swarm from backup. Retrieve the unlock key if necessary and
store it in a safe location. If you are unsure, read
<a href="/engine/swarm/swarm_manager_locking/">Lock your swarm to protect its encryption key</a>.</p>
  </li>
  <li>
    <p>Stop Docker on the manager before backing up the data, so that no data is
being changed during the backup. It is possible to take a backup while the
manager is running (a “hot” backup), but this is not recommended and your
results are less predictable when restoring. While the manager is down,
other nodes continue generating swarm data that is not part of this backup.</p>

    <blockquote>
      <p>Note</p>

      <p>Be sure to maintain the quorum of swarm managers. During the
time that a manager is shut down, your swarm is more vulnerable to
losing the quorum if further nodes are lost. The number of managers you
run is a trade-off. If you regularly take down managers to do backups,
consider running a five manager swarm, so that you can lose an additional
manager while the backup is running, without disrupting your services.</p>
    </blockquote>
  </li>
  <li>
    <p>Back up the entire <code class="highlighter-rouge">/var/lib/docker/swarm</code> directory.</p>
  </li>
  <li>
    <p>Restart the manager.</p>
  </li>
</ol>

<p>To restore, see <a href="#restore-from-a-backup">Restore from a backup</a>.</p>

<h2 id="recover-from-disaster">Recover from disaster</h2>

<h3 id="restore-from-a-backup">Restore from a backup</h3>

<p>After backing up the swarm as described in
<a href="#back-up-the-swarm">Back up the swarm</a>, use the following procedure to
restore the data to a new swarm.</p>

<ol>
  <li>
    <p>Shut down Docker on the target host machine for the restored swarm.</p>
  </li>
  <li>
    <p>Remove the contents of the <code class="highlighter-rouge">/var/lib/docker/swarm</code> directory on the new
swarm.</p>
  </li>
  <li>
    <p>Restore the <code class="highlighter-rouge">/var/lib/docker/swarm</code> directory with the contents of the
backup.</p>

    <blockquote>
      <p>Note</p>

      <p>The new node uses the same encryption key for on-disk
storage as the old one. It is not possible to change the on-disk storage
encryption keys at this time.</p>

      <p>In the case of a swarm with auto-lock enabled, the unlock key is also the
same as on the old swarm, and the unlock key is needed to restore the
swarm.</p>
    </blockquote>
  </li>
  <li>
    <p>Start Docker on the new node. Unlock the swarm if necessary. Re-initialize
the swarm using the following command, so that this node does not attempt
to connect to nodes that were part of the old swarm, and presumably no
longer exist.</p>

    <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span> docker swarm init <span class="nt">--force-new-cluster</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>Verify that the state of the swarm is as expected. This may include
application-specific tests or simply checking the output of
<code class="highlighter-rouge">docker service ls</code> to be sure that all expected services are present.</p>
  </li>
  <li>
    <p>If you use auto-lock,
<a href="/engine/swarm/swarm_manager_locking/#rotate-the-unlock-key">rotate the unlock key</a>.</p>
  </li>
  <li>
    <p>Add manager and worker nodes to bring your new swarm up to operating
capacity.</p>
  </li>
  <li>
    <p>Reinstate your previous backup regimen on the new swarm.</p>
  </li>
</ol>

<h3 id="recover-from-losing-the-quorum">Recover from losing the quorum</h3>

<p>Swarm is resilient to failures and the swarm can recover from any number
of temporary node failures (machine reboots or crash with restart) or other
transient errors. However, a swarm cannot automatically recover if it loses a
quorum. Tasks on existing worker nodes continue to run, but administrative
tasks are not possible, including scaling or updating services and joining or
removing nodes from the swarm. The best way to recover is to bring the missing
manager nodes back online. If that is not possible, continue reading for some
options for recovering your swarm.</p>

<p>In a swarm of <code class="highlighter-rouge">N</code> managers, a quorum (a majority) of manager nodes must always
be available. For example, in a swarm with five managers, a minimum of three must be
operational and in communication with each other. In other words, the swarm can
tolerate up to <code class="highlighter-rouge">(N-1)/2</code> permanent failures beyond which requests involving
swarm management cannot be processed. These types of failures include data
corruption or hardware failures.</p>

<p>If you lose the quorum of managers, you cannot administer the swarm. If you have
lost the quorum and you attempt to perform any management operation on the swarm,
an error occurs:</p>

<pre><code class="language-none">Error response from daemon: rpc error: code = 4 desc = context deadline exceeded
</code></pre>

<p>The best way to recover from losing the quorum is to bring the failed nodes back
online. If you can’t do that, the only way to recover from this state is to use
the <code class="highlighter-rouge">--force-new-cluster</code> action from a manager node. This removes all managers
except the manager the command was run from. The quorum is achieved because
there is now only one manager. Promote nodes to be managers until you have the
desired number of managers.</p>

<p>From the node to recover, run:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span> docker swarm init <span class="nt">--force-new-cluster</span> <span class="nt">--advertise-addr</span> node01:2377
</code></pre></div></div>

<p>When you run the <code class="highlighter-rouge">docker swarm init</code> command with the <code class="highlighter-rouge">--force-new-cluster</code>
flag, the Docker Engine where you run the command becomes the manager node of a
single-node swarm which is capable of managing and running services. The manager
has all the previous information about services and tasks, worker nodes are
still part of the swarm, and services are still running. You need to add or
re-add  manager nodes to achieve your previous task distribution and ensure that
you have enough managers to maintain high availability and prevent losing the
quorum.</p>

<h2 id="force-the-swarm-to-rebalance">Force the swarm to rebalance</h2>

<p>Generally, you do not need to force the swarm to rebalance its tasks. When you
add a new node to a swarm, or a node reconnects to the swarm after a
period of unavailability, the swarm does not automatically give a workload to
the idle node. This is a design decision. If the swarm periodically shifted tasks
to different nodes for the sake of balance, the clients using those tasks would
be disrupted. The goal is to avoid disrupting running services for the sake of
balance across the swarm. When new tasks start, or when a node with running
tasks becomes unavailable, those tasks are given to less busy nodes. The goal
is eventual balance, with minimal disruption to the end user.</p>

<p>You can use the <code class="highlighter-rouge">--force</code> or <code class="highlighter-rouge">-f</code> flag with the <code class="highlighter-rouge">docker service update</code> command
to force the service to redistribute its tasks across the available worker nodes.
This causes the service tasks to restart. Client applications may be disrupted.
If you have configured it, your service uses a <a href="/engine/swarm/swarm-tutorial/rolling-update/">rolling update</a>.</p>

<p>If you use an earlier version and you want to achieve an even balance of load
across workers and don’t mind disrupting running tasks, you can force your swarm
to re-balance by temporarily scaling the service upward. Use
<code class="highlighter-rouge">docker service inspect --pretty &lt;servicename&gt;</code> to see the configured scale
of a service. When you use <code class="highlighter-rouge">docker service scale</code>, the nodes with the lowest
number of tasks are targeted to receive the new workloads. There may be multiple
under-loaded nodes in your swarm. You may need to scale the service up by modest
increments a few times to achieve the balance you want across all the nodes.</p>

<p>When the load is balanced to your satisfaction, you can scale the service back
down to the original scale. You can use <code class="highlighter-rouge">docker service ps</code> to assess the current
balance of your service across nodes.</p>

<p>See also
<a href="/engine/reference/commandline/service_scale/"><code class="highlighter-rouge">docker service scale</code></a> and
<a href="/engine/reference/commandline/service_ps/"><code class="highlighter-rouge">docker service ps</code></a>.</p>
<span class="glyphicon glyphicon-tags" style="padding-right: 10px"></span><span style="vertical-align: 2px"><a href="/search/?q=docker">docker</a>, <a href="/search/?q=container">container</a>, <a href="/search/?q=swarm">swarm</a>, <a href="/search/?q=manager">manager</a>, <a href="/search/?q=raft">raft</a></span><div class="ratings-div"><div id="pd_rating_holder_8453675"></div></div></section>
                    </main>
                    <nav class="col-nav">
                        <div id="sidebar-nav" class="sidebar hidden-sm hidden-xs">
                            <div id="navbar" class="nav-sidebar">
                                <ul class="nav hidden-md hidden-lg"></ul>
                                <ul class="nav" id="jsTOCLeftNav"></ul>
                            </div>
                        </div>
                    </nav>
                    <div class="col-toc">
                        <div class="sidebar hidden-xs hidden-sm">
                            <div class="toc-nav">
                                <div class="feedback-links">
                                    <ul><li><a href="https://github.com/docker/docker.github.io/edit/master/engine/swarm/admin_guide.md"><i class="fa fa-pencil-square-o" aria-hidden="true"></i> Edit this page</a></li><li><a href="https://github.com/docker/docker.github.io/issues/new?body=File: [engine/swarm/admin_guide.md](https://docs.docker.com/engine/swarm/admin_guide/)" class="nomunge"><i class="fa fa-check" aria-hidden="true"></i> Request docs changes</a></li>
                                        <li><div class="toggle-mode">
  <div class="icon">
      <i class="fa fa-sun-o" aria-hidden="true"></i>
  </div>
  <div class="toggle-switch">
      <label class="switch">
          <input type="checkbox" id="switch-style">
          <span class="slider round"></span>
      </label>
  </div>
  <div class="icon">
      <i class="fa fa-moon-o" aria-hidden="true"></i>
  </div>
</div>
</li>
                                    </ul>
                                </div><div id="side-toc-title">On this page:</div>
<ul id="my_toc" class="inline_toc">
  <li><a href="#operate-manager-nodes-in-a-swarm" class="nomunge">Operate manager nodes in a swarm</a>
    <ul>
      <li><a href="#maintain-the-quorum-of-managers" class="nomunge">Maintain the quorum of managers</a></li>
    </ul>
  </li>
  <li><a href="#configure-the-manager-to-advertise-on-a-static-ip-address" class="nomunge">Configure the manager to advertise on a static IP address</a></li>
  <li><a href="#add-manager-nodes-for-fault-tolerance" class="nomunge">Add manager nodes for fault tolerance</a>
    <ul>
      <li><a href="#distribute-manager-nodes" class="nomunge">Distribute manager nodes</a></li>
      <li><a href="#run-manager-only-nodes" class="nomunge">Run manager-only nodes</a></li>
    </ul>
  </li>
  <li><a href="#add-worker-nodes-for-load-balancing" class="nomunge">Add worker nodes for load balancing</a></li>
  <li><a href="#monitor-swarm-health" class="nomunge">Monitor swarm health</a></li>
  <li><a href="#troubleshoot-a-manager-node" class="nomunge">Troubleshoot a manager node</a></li>
  <li><a href="#forcibly-remove-a-node" class="nomunge">Forcibly remove a node</a></li>
  <li><a href="#back-up-the-swarm" class="nomunge">Back up the swarm</a></li>
  <li><a href="#recover-from-disaster" class="nomunge">Recover from disaster</a>
    <ul>
      <li><a href="#restore-from-a-backup" class="nomunge">Restore from a backup</a></li>
      <li><a href="#recover-from-losing-the-quorum" class="nomunge">Recover from losing the quorum</a></li>
    </ul>
  </li>
  <li><a href="#force-the-swarm-to-rebalance" class="nomunge">Force the swarm to rebalance</a></li>
</ul>

</div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <footer class="footer">
          
    <div class="container">
        <div class="top_footer">
            <div class="row">
                <div class="col-xs-12 col-sm-3 col-md-3">
                    <ul class="footer_links">
                        <li><b><a href="https://www.docker.com/why-docker">Why Docker?</a></b></li>
                        <li><a href="https://www.docker.com/what-container">What is a Container?</a></li>
                        <li><a href="https://docker.events.cube365.net/dockercon/">DockerCon</a></li>
                        <li><b><a href="https://www.docker.com/products/overview">Products</a></b></li>
                        <li><a href="https://www.docker.com/products/docker-desktop">Docker Desktop</a></li>
                        <li><a href="https://www.docker.com/products/docker-hub">Docker Hub</a></li>
                        <li><a href="https://www.docker.com/roadmap">Docker Product Roadmap</a></li>
                        <li><b><a href="https://www.docker.com/products/docker-desktop">Features</a></b></li>
                        <li><a href="https://www.docker.com/products/container-runtime">Container Runtime</a></li>
                        <li><a href="https://www.docker.com/products/developer-tools">Developer Tools</a></li>
                    </ul>
                </div>
                <div class="col-xs-12 col-sm-3 col-md-3">
                    <ul class="footer_links">
                        <li><b><a href="https://www.docker.com/products/docker-desktop">Developers</a></b></li>
                        <li><a href="https://www.docker.com/use-cases">Use Cases</a></li>
                        <li><a href="https://www.docker.com/docker-community">Community</a></li>
                        <li><a href="https://www.docker.com/open-source">Open Source</a></li>
                        <li><a href="https://www.docker.com/community/docker-captains">Docker Captains</a></li>
                    </ul>
                </div>
                <div class="col-xs-12 col-sm-3 col-md-3">
                    <ul class="footer_links">
                        <li><b><a href="https://www.docker.com/products/docker-desktop">Pricing</a></b></li>
                        <li><a href="https://www.docker.com/pricing/faq">FAQs</a></li>
                        <li><a href="https://www.docker.com/partners/programs">Docker Verified Publisher Program</a></li>
                    </ul>
                </div>
                <div class="col-xs-12 col-sm-3 col-md-3">
                    <ul class="footer_links">
                        <li><b><a href="https://www.docker.com/company" target="_blank" rel="noopener">Company</a></b></li>
                        <li><a href="https://www.docker.com/company">About Us</a></li>
                        <li><a href="https://www.docker.com/blog/" target="_blank" rel="noopener">Blog</a></li>
                        <li><a href="https://www.docker.com/customers">Customers</a></li>
                        <li><a href="https://www.docker.com/partners">Partners</a></li>
                        <li><a href="https://www.docker.com/company/newsroom">Newsroom</a></li>
                        <li><a href="https://www.docker.com/careers">Careers</a></li>
                        <li><a href="https://www.docker.com/company/contact">Contact Us</a></li>
                    </ul>
                </div>
            </div>
            <div class="footer-nav">
                <nav class="footer_sub_nav">
                    <ul class="menu">
                        <li><a href="http://status.docker.com/">Status</a></li>
                        <li><a href="https://www.docker.com/legal">Legal</a></li>
                        <li><a href="https://www.docker.com/company/contact">Contact</a></li>
                    </ul>
                </nav>
            </div>
        </div>
        <div class="bottom_footer">
            <div class="footer-copyright col-xs-12 col-md-8">
                <p class="copyright">
                    Copyright &copy; 2013-2021 Docker Inc. All rights reserved. </p>
            </div>
            <div class="footer_social_nav">
                <ul class="nav-social">
                    <li class="fa fa-twitter"><a href="http://twitter.com/docker">Twitter</a></li>
                    <li class="fa fa-youtube"><a href="http://www.youtube.com/user/dockerrun">Youtube</a></li>
                    <li class="fa fa-github"><a href="https://github.com/docker">GitHub</a></li>
                    <li class="fa fa-linkedin"><a href="https://www.linkedin.com/company/docker">Linkedin</a></li>
                    <li class="fa fa-facebook"><a href="https://www.facebook.com/docker.run">Facebook</a></li>
                    <li class="fa fa-slideshare"><a href="https://www.slideshare.net/docker">Slideshare</a></li>
                    <li class="fa fa-reddit"><a href="https://www.reddit.com/r/docker">Reddit</a></li>
                </ul>
            </div>
        </div>
    </div>

    </footer>
    <script>const pageURL = "/engine/swarm/admin_guide/";</script><script>
    let PDRTJS_settings_8453675 = {
        "id": "8453675",
        "unique_id": "engine/swarm/admin_guide.md",
        "title": "Administer and maintain a swarm of Docker Engines",
        "font_family": "Open Sans, sans serif",
        "font_color": "b9c2cc",
        "font_align": "center",
        "permalink": "https://github.com/docker/docker.github.io/blob/master/engine/swarm/admin_guide.md"
    };
    (function (d, c, j) {
        if (!document.getElementById(j)) {
            let pd=d.createElement(c),s;pd.id=j;pd.src='https://polldaddy.com/js/rating/rating.js';s=document.getElementsByTagName(c)[0];s.parentNode.insertBefore(pd, s);
        }
    }(document, 'script', 'pd-rating-js'));
</script></body>
</html>
